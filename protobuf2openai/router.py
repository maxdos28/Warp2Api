from __future__ import annotations

import asyncio
import json
import time
import uuid
from copy import deepcopy
from typing import Any, Dict, List, Optional

import httpx
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse

from .logging import logger

from .models import ChatCompletionsRequest, ChatMessage
from .reorder import reorder_messages_for_anthropic
from .helpers import normalize_content_to_list, segments_to_text
from .packets import packet_template, map_history_to_warp_messages, attach_user_and_tools_to_inputs
from .state import STATE
from .config import BRIDGE_BASE_URL
from .bridge import initialize_once
from .sse_transform import stream_openai_sse
from .auth import authenticate_request
from .http_clients import get_shared_async_client, PerformanceTracker, get_performance_metrics
from .cache import cache_get, cache_set, CacheableRequest, cache_stats
from .performance_monitor import get_performance_summary
from .memory_optimizer import get_memory_stats
from .request_batcher import get_batch_stats
from .async_logging import get_async_log_stats
from .rate_limiter import get_rate_limit_stats
from .circuit_breaker import get_all_circuit_breaker_stats
from .json_optimizer import get_json_stats
from .compression import get_compression_stats
from .quota_handler import check_request_throttling, get_quota_handler
from .cost_handler import record_api_cost, extract_and_format_cost, get_cost_stats
from .simple_response_handler import create_simple_chat_response, extract_response_from_bridge, is_valid_response
from .direct_response import handle_chat_request_directly


router = APIRouter()


def _normalize_message_content_for_dedup(msg: ChatMessage) -> str:
    """ç”Ÿæˆç”¨äºå»é‡çš„æ¶ˆæ¯å†…å®¹è§„èŒƒåŒ–å­—ç¬¦ä¸²ï¼Œå¿½ç•¥å·¥å…·è°ƒç”¨ä¸­çš„åŠ¨æ€å­—æ®µ"""
    content = getattr(msg, "content", None)

    if isinstance(content, str):
        return content.strip()

    if isinstance(content, list):
        normalized_blocks: List[Any] = []

        for block in content:
            if isinstance(block, dict):
                block_copy = deepcopy(block)

                block_type = block_copy.get("type")

                if block_type == "tool_use":
                    block_copy.pop("id", None)
                if block_type == "tool_result":
                    block_copy.pop("tool_use_id", None)

                try:
                    normalized_blocks.append(json.loads(json.dumps(block_copy, sort_keys=True)))
                except Exception:
                    normalized_blocks.append(block_copy)
            else:
                normalized_blocks.append(block)

        try:
            return json.dumps(normalized_blocks, ensure_ascii=False, sort_keys=True)
        except Exception:
            return str(normalized_blocks)

    return str(content)


def _deduplicate_messages(messages: List[ChatMessage]) -> List[ChatMessage]:
    """ç§»é™¤é‡å¤çš„æ¶ˆæ¯ï¼Œç‰¹åˆ«æ˜¯ç³»ç»Ÿæç¤ºå’Œé‡å¤çš„ç”¨æˆ·æ¶ˆæ¯ï¼Œä½†ä¿ç•™æœ€åä¸€æ¡æ¶ˆæ¯"""
    if not messages:
        return messages
    
    # å¦‚æœåªæœ‰ä¸€æ¡æ¶ˆæ¯ï¼Œç›´æ¥è¿”å›
    if len(messages) == 1:
        return messages
    
    seen_content = set()
    deduplicated = []
    
    # å¤„ç†é™¤æœ€åä¸€æ¡æ¶ˆæ¯å¤–çš„æ‰€æœ‰æ¶ˆæ¯
    for i, msg in enumerate(messages[:-1]):
        # å®‰å…¨åœ°å¤„ç†contentå­—æ®µï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
        content_str = _normalize_message_content_for_dedup(msg)
        
        # ç‰¹æ®Šæ£€æŸ¥ï¼šå¦‚æœåŒ…å«é‡å¤çš„åˆ†ææ¨¡å¼ï¼Œç›´æ¥è·³è¿‡
        if _is_repetitive_analysis_content(content_str):
            logger.debug(f"[OpenAI Compat] Skipping repetitive analysis content")
            continue
        
        # æ¸…ç†å†…å®¹ï¼Œç§»é™¤å¸¸è§çš„é‡å¤æ¨¡å¼
        cleaned_content = _clean_content_for_dedup(content_str)
        
        # åˆ›å»ºæ¶ˆæ¯çš„å”¯ä¸€æ ‡è¯†
        content_key = f"{msg.role}:{cleaned_content[:300]}"  # ä½¿ç”¨å‰300ä¸ªå­—ç¬¦ä½œä¸ºæ ‡è¯†
        
        if content_key not in seen_content:
            seen_content.add(content_key)
            deduplicated.append(msg)
        else:
            logger.debug(f"[OpenAI Compat] Removed duplicate message: {content_key}")
    
    # æ€»æ˜¯ä¿ç•™æœ€åä¸€æ¡æ¶ˆæ¯
    deduplicated.append(messages[-1])
    
    logger.info(f"[OpenAI Compat] Deduplicated messages: {len(messages)} -> {len(deduplicated)}")
    return deduplicated


def _is_repetitive_analysis_content(content: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦åŒ…å«é‡å¤çš„åˆ†æå†…å®¹"""
    if not content:
        return False
    
    # æ£€æŸ¥ç‰¹å®šçš„é‡å¤æ¨¡å¼
    repetitive_patterns = [
        "I'll analyze your" in content and "create a comprehensive" in content and "Let me start by exploring" in content,
        "Let me start by exploring" in content and "codebase structure and key files" in content and "è¿˜æ˜¯è¿™æ ·å•Š" in content,
        "I'll analyze your" in content and "Let me start by exploring" in content and "codebase structure" in content,
        "I'll analyze your" in content and "create a comprehensive" in content and "CLAUDE.md" in content and "Let me start by exploring" in content,
        "I'll analyze your" in content and "create a comprehensive" in content and "Let me start by exploring" in content and "codebase structure and key files" in content,
        "I'll analyze your" in content and "create a comprehensive" in content and "Let me start by exploring" in content and "codebase structure and key files" in content and "è¿˜æ˜¯è¿™æ ·å•Š" in content,
        # æ·»åŠ é’ˆå¯¹ä¸­æ–‡é‡å¤è¯·æ±‚çš„æ£€æµ‹
        "æ‚¨è¯´å¾—å¯¹" in content and "æˆ‘ç¡®å®éœ€è¦ä¸ºPHPç‰ˆæœ¬ä¹Ÿå®ç°" in content,
        "æ‚¨è¯´å¾—å®Œå…¨æ­£ç¡®" in content and "æˆ‘ç¡®å®é—æ¼äº†PHPç‰ˆæœ¬çš„å®ç°" in content,
        "è®©æˆ‘å…ˆæŸ¥çœ‹PHPç‰ˆæœ¬" in content and "æ§åˆ¶å™¨ä»£ç " in content,
        "è®©æˆ‘æŸ¥çœ‹PHPç‰ˆæœ¬çš„å‘å¸ƒå•æ§åˆ¶å™¨ä»£ç " in content,
    ]
    
    return any(repetitive_patterns)


def _clean_content_for_dedup(content: str) -> str:
    """æ¸…ç†å†…å®¹ä»¥ä¾¿æ›´å¥½åœ°è¯†åˆ«é‡å¤"""
    if not content:
        return ""
    
    # ç§»é™¤å¸¸è§çš„é‡å¤æ¨¡å¼
    cleaned = content
    
    # ç§»é™¤é‡å¤çš„åˆ†ææ­¥éª¤å’Œå‘½ä»¤
    patterns_to_remove = [
        r"I'll analyze this codebase.*?Let me start by examining",
        r"Let me start by examining.*?I'll analyze the codebase", 
        r"You're right! Let me use the correct tools.*?I'll start by checking",
        r"Let me try the correct format.*?claude /init",
        r"I'll analyze this codebase.*?create.*?CLAUDE\.md",
        r"Let me start by examining.*?project structure",
        r"Let me use the correct tools.*?analyze the codebase",
        r"Let me try the correct format.*?file_glob",
        r"claude /init.*?è¾“å‡º.*?è¿˜æ˜¯æœ‰é—®é¢˜",
        r"Let me start by examining.*?key files",
        r"I'll start by checking.*?existing CLAUDE\.md",
        r"Let me try the correct format.*?file_glob:",
        r"You're absolutely right! Let me use the correct tools.*?I'll start by checking",
        r"Let me correct my previous tool call.*?Now let me examine",
        r"Let me check for an existing CLAUDE\.md.*?using the correct tools",
        r"Let me correct my file_glob call.*?cluade code",
        r"I'll analyze the codebase.*?create a CLAUDE\.md file",
        r"Let me start by examining.*?project structure and key files",
        r"You're absolutely right!.*?Let me use the correct tools",
        r"Let me correct my previous.*?Now let me examine",
        r"Let me check for.*?existing files",
        r"Let me correct my.*?file_glob call",
        r"Now let me examine.*?project structure",
        r"Let me check for an existing.*?CLAUDE\.md file",
        r"Let me correct my file_glob.*?cluade code",
        r"I'll analyze your.*?codebase.*?create.*?CLAUDE\.md",
        r"Let me start by exploring.*?codebase structure",
        r"I'll analyze your.*?Let me start by exploring",
        r"Let me start by exploring.*?I'll analyze your",
        r"Let me start by exploring.*?codebase structure and key files",
        r"I'll analyze your.*?codebase.*?Let me start by exploring",
        r"Let me start by exploring.*?codebase structure.*?key files",
        r"I'll analyze your.*?codebase.*?create.*?comprehensive.*?CLAUDE\.md",
        r"Let me start by exploring.*?codebase structure.*?key files.*?è¿˜æ˜¯è¿™æ ·å•Š",
        r"I'll analyze your.*?codebase.*?create.*?comprehensive.*?CLAUDE\.md.*?Let me start by exploring",
        r"Let me start by exploring.*?codebase structure.*?key files.*?è¿˜æ˜¯è¿™æ ·å•Š",
        r"I'll analyze your.*?codebase.*?create.*?comprehensive.*?CLAUDE\.md.*?Let me start by exploring.*?codebase structure.*?key files.*?è¿˜æ˜¯è¿™æ ·å•Š",
        # æ·»åŠ ä¸­æ–‡é‡å¤æ¨¡å¼çš„æ¸…ç†
        r"æ‚¨è¯´å¾—å¯¹.*?æˆ‘ç¡®å®éœ€è¦ä¸ºPHPç‰ˆæœ¬.*?å®ç°.*?å‘å¸ƒå•.*?é™åˆ¶",
        r"æ‚¨è¯´å¾—å®Œå…¨æ­£ç¡®.*?æˆ‘ç¡®å®é—æ¼äº†PHPç‰ˆæœ¬.*?å®ç°",
        r"è®©æˆ‘.*?æŸ¥çœ‹.*?PHPç‰ˆæœ¬.*?å‘å¸ƒå•æ§åˆ¶å™¨",
        r"æ‚¨è¯´å¾—æ­£ç¡®.*?æˆ‘ç¡®å®é—æ¼äº†PHPç‰ˆæœ¬",
        r"Cline wants to read this file.*?ReleaseSheetController\.php",
    ]
    
    import re
    for pattern in patterns_to_remove:
        cleaned = re.sub(pattern, "", cleaned, flags=re.DOTALL)
    
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªé‡å¤çš„åˆ†ææ­¥éª¤
    analysis_phrases = [
        "I'll analyze",
        "Let me start by examining", 
        "Let me use the correct tools",
        "Let me check for",
        "Let me correct my",
        "Now let me examine",
        "You're absolutely right",
        "Let me try the correct format",
        "Let me start by exploring",
        "I'll analyze your",
        "create a comprehensive",
        "codebase structure",
        "key files",
        "è¿˜æ˜¯è¿™æ ·å•Š"
    ]
    
    phrase_count = sum(1 for phrase in analysis_phrases if phrase in cleaned)
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡å¤çš„åˆ†ææ¨¡å¼
    repetitive_patterns = [
        "I'll analyze" in cleaned and "Let me start by" in cleaned,
        "Let me start by" in cleaned and "codebase" in cleaned and "CLAUDE.md" in cleaned,
        "Let me start by exploring" in cleaned and "codebase structure" in cleaned,
        "I'll analyze your" in cleaned and "Let me start by exploring" in cleaned,
        "è¿˜æ˜¯è¿™æ ·å•Š" in cleaned and ("I'll analyze" in cleaned or "Let me start by" in cleaned),
        "I'll analyze your" in cleaned and "create a comprehensive" in cleaned and "CLAUDE.md" in cleaned,
        "Let me start by exploring" in cleaned and "codebase structure and key files" in cleaned,
        "I'll analyze your" in cleaned and "Let me start by exploring" in cleaned and "codebase structure" in cleaned,
        "I'll analyze your" in cleaned and "create a comprehensive" in cleaned and "Let me start by exploring" in cleaned,
        "Let me start by exploring" in cleaned and "codebase structure and key files" in cleaned and "è¿˜æ˜¯è¿™æ ·å•Š" in cleaned
    ]
    
    # å¦‚æœåŒ…å«å¤ªå¤šåˆ†æçŸ­è¯­æˆ–é‡å¤æ¨¡å¼ï¼Œå¯èƒ½æ˜¯é‡å¤å†…å®¹
    if phrase_count >= 3 or any(repetitive_patterns):
        logger.debug(f"[OpenAI Compat] Detected repetitive analysis content with {phrase_count} phrases and patterns: {repetitive_patterns}")
        return ""
    
    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœåŒ…å«ç‰¹å®šçš„é‡å¤æ¨¡å¼ï¼Œç›´æ¥è¿”å›ç©ºå­—ç¬¦ä¸²
    if ("I'll analyze your" in cleaned and "create a comprehensive" in cleaned and 
        "Let me start by exploring" in cleaned and "codebase structure and key files" in cleaned):
        logger.debug(f"[OpenAI Compat] Detected specific repetitive pattern, filtering out")
        return ""
    
    # å¦‚æœæ¸…ç†åå†…å®¹å¤ªçŸ­ï¼Œå¯èƒ½æ˜¯é‡å¤å†…å®¹ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
    if len(cleaned) < 10:
        return ""
    
    return cleaned


def _get_tool_response_text(tool_name: str, tool_args: Dict[str, Any]) -> str:
    """æ ¹æ®å·¥å…·ç±»å‹ç”Ÿæˆé€‚å½“çš„å“åº”æ–‡æœ¬"""
    if tool_name == "read_file":
        file_path = tool_args.get("path", "the file")
        return f"I'll help you examine {file_path}. Let me read that file for you."
    elif tool_name in ["list_files", "ls", "dir"]:
        path = tool_args.get("path", "the current directory")
        return f"I'll list the files in {path} for you."
    elif tool_name in ["create_file", "write_file"]:
        file_path = tool_args.get("path", "the file")
        return f"I'll create {file_path} for you."
    elif tool_name in ["run_command", "execute", "bash"]:
        command = tool_args.get("command", "the command")
        return f"I'll execute the command: {command}"
    elif tool_name in ["search", "grep"]:
        pattern = tool_args.get("pattern", "the pattern")
        return f"I'll search for '{pattern}' in the codebase."
    else:
        return f"I'll execute {tool_name} with the provided arguments."


@router.get("/")
def root():
    return {"service": "OpenAI Chat Completions (Warp bridge) - Streaming", "status": "ok"}


@router.get("/healthz")
def health_check():
    return {"status": "ok", "service": "OpenAI Chat Completions (Warp bridge) - Streaming"}


@router.get("/v1/models")
async def list_models():
    """OpenAI-compatible model listing. Forwards to bridge, with local fallback."""
    logger.info("[OpenAI Compat] Models endpoint called")
    
    # ç›´æ¥è¿”å›ç¡¬ç¼–ç çš„æ¨¡å‹åˆ—è¡¨ï¼Œé¿å…å¤æ‚çš„æ¡¥æ¥é—®é¢˜
    models_data = {
        "object": "list",
        "data": [
            {
                "id": "claude-4-sonnet",
                "object": "model", 
                "created": 1677610602,
                "owned_by": "anthropic",
                "permission": [],
                "root": "claude-4-sonnet"
            },
            {
                "id": "claude-4-opus",
                "object": "model",
                "created": 1677610602, 
                "owned_by": "anthropic",
                "permission": [],
                "root": "claude-4-opus"
            },
            {
                "id": "claude-4.1-opus",
                "object": "model",
                "created": 1677610602,
                "owned_by": "anthropic", 
                "permission": [],
                "root": "claude-4.1-opus"
            },
            {
                "id": "gemini-2.5-pro",
                "object": "model",
                "created": 1677610602,
                "owned_by": "google",
                "permission": [],
                "root": "gemini-2.5-pro"
            },
            {
                "id": "gpt-4.1",
                "object": "model",
                "created": 1677610602,
                "owned_by": "openai",
                "permission": [],
                "root": "gpt-4.1"
            },
            {
                "id": "gpt-4o", 
                "object": "model",
                "created": 1677610602,
                "owned_by": "openai",
                "permission": [],
                "root": "gpt-4o"
            },
            {
                "id": "gpt-5",
                "object": "model", 
                "created": 1677610602,
                "owned_by": "openai",
                "permission": [],
                "root": "gpt-5"
            },
            {
                "id": "o3",
                "object": "model",
                "created": 1677610602,
                "owned_by": "openai", 
                "permission": [],
                "root": "o3"
            },
            {
                "id": "o4-mini",
                "object": "model",
                "created": 1677610602,
                "owned_by": "openai",
                "permission": [],
                "root": "o4-mini"
            }
        ]
    }
    
    logger.info(f"[OpenAI Compat] Returning {len(models_data['data'])} models")
    return models_data


@router.post("/v1/chat/completions")
async def chat_completions(req: ChatCompletionsRequest, request: Request = None):
    request_start_time = time.time()
    
    # ğŸ”¥ ç´§æ€¥ä¿®å¤ï¼šç›´æ¥å¤„ç†æ‰€æœ‰è¯·æ±‚ï¼Œç»•è¿‡å¤æ‚çš„SSEé€»è¾‘
    logger.info("[OpenAI Compat] Using DIRECT response handler to fix Cline issues")
    try:
        request_dict = {
            "model": req.model,
            "messages": [{"role": msg.role, "content": msg.content} for msg in req.messages],
            "stream": req.stream,
            "max_tokens": getattr(req, 'max_tokens', 1000),
            "temperature": getattr(req, 'temperature', 0.7)
        }
        
        direct_response = await handle_chat_request_directly(request_dict)
        
        if req.stream and hasattr(direct_response, '__aiter__'):
            return StreamingResponse(direct_response, media_type="text/event-stream")
        else:
            return direct_response
            
    except Exception as direct_error:
        logger.error(f"[OpenAI Compat] Direct handler failed: {direct_error}, falling back to complex logic")
    
    # å¦‚æœç›´æ¥å¤„ç†å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸæ¥çš„é€»è¾‘
    # æ£€æŸ¥è¯·æ±‚æ˜¯å¦åº”è¯¥è¢«é™åˆ¶ï¼ˆé…é¢/é€Ÿç‡é™åˆ¶ï¼‰
    throttle_response = await check_request_throttling()
    if throttle_response:
        logger.warning(f"[OpenAI Compat] Request throttled: {throttle_response}")
        return throttle_response
    
    # æ³¨æ„ï¼šä¸åº”è¯¥åœ¨è¯·æ±‚æ¶ˆæ¯ä¸­æ£€æŸ¥"high demand"ï¼Œå› ä¸ºç”¨æˆ·å¯èƒ½åˆç†åœ°æåˆ°è¿™ä¸ªè¯
    # high demandæ£€æŸ¥åº”è¯¥åœ¨å“åº”å¤„ç†é˜¶æ®µè¿›è¡Œ
    
    # è¯¦ç»†è®°å½•è¯·æ±‚ä¿¡æ¯ç”¨äºè°ƒè¯•Clineé—®é¢˜
    logger.info(f"[OpenAI Compat] ===== NEW CHAT COMPLETIONS REQUEST =====")
    logger.info(f"[OpenAI Compat] Model: {req.model}")
    logger.info(f"[OpenAI Compat] Stream: {req.stream}")
    logger.info(f"[OpenAI Compat] Messages count: {len(req.messages) if req.messages else 0}")
    logger.info(f"[OpenAI Compat] Tools: {len(req.tools) if req.tools else 0}")
    if request:
        logger.info(f"[OpenAI Compat] User-Agent: {request.headers.get('user-agent', 'unknown')}")
        logger.info(f"[OpenAI Compat] Client IP: {request.client.host if request.client else 'unknown'}")
    logger.info(f"[OpenAI Compat] ==========================================")
    
    # ç›´æ¥å†…è”æ£€æŸ¥ Cline è¯·æ±‚ï¼ˆé¿å…å¯¼å…¥é—®é¢˜ï¼‰
    is_cline = False
    file_path = None
    
    # è®°å½•æ‰€æœ‰ç”¨æˆ·æ¶ˆæ¯å†…å®¹ä»¥ä¾¿è°ƒè¯•
    for i, msg in enumerate(req.messages):
        if msg.role == "user" and isinstance(msg.content, str):
            content = msg.content
            logger.info(f"[OpenAI Compat] User message {i}: {content[:200]}...")
            
            # æ£€æŸ¥å¤šç§å¯èƒ½çš„ Cline æ ‡è¯†
            cline_markers = [
                "Cline wants to read this file:",
                "cline wants to read",
                "read this file",
                "Controller.php",
                "ReleaseSheet"
            ]
            
            for marker in cline_markers:
                if marker.lower() in content.lower():
                    is_cline = True
                    logger.info(f"[OpenAI Compat] Found Cline marker '{marker}' in message!")
                    
                    # å°è¯•æå–æ–‡ä»¶è·¯å¾„
                    if "Controller.php" in content:
                        import re
                        # æŸ¥æ‰¾ä»»ä½• .php æ–‡ä»¶è·¯å¾„
                        php_match = re.search(r'([/\\]?[\w/\\.-]*Controller\.php)', content)
                        if php_match:
                            file_path = php_match.group(1)
                            logger.info(f"[OpenAI Compat] Extracted file path: {file_path}")
                    break
            
            if is_cline:
                break
    
    # å¦‚æœè¿˜æ²¡æœ‰æ£€æµ‹åˆ°ï¼Œä½†æ˜¯è¯·æ±‚ä¸­åŒ…å«ä»£ç ç›¸å…³å†…å®¹ï¼Œå‡è®¾æ˜¯ IDE è¯·æ±‚
    if not is_cline:
        for msg in req.messages:
            if msg.role == "user" and isinstance(msg.content, str):
                content = msg.content.lower()
                if any(keyword in content for keyword in [
                    "php", "å‘å¸ƒå•", "é™åˆ¶", "controller", "release",
                    "æŸ¥çœ‹", "å®ç°", "ä»£ç ", "æ–‡ä»¶"
                ]):
                    is_cline = True
                    file_path = None  # ä½¿ç”¨é»˜è®¤æ–‡ä»¶è·¯å¾„
                    logger.info(f"[OpenAI Compat] Fallback: Detected code-related content, assuming Cline request")
                    break
    
    if is_cline:
        logger.info(f"[OpenAI Compat] Cline request detected! File path: {file_path}")
        
        # ç›´æ¥è¿”å›å·¥å…·è°ƒç”¨å“åº”
        completion_id = str(uuid.uuid4())
        created_ts = int(time.time())
        model_id = req.model or "claude-4-sonnet"
        
        # åˆ›å»ºå·¥å…·è°ƒç”¨å“åº”
        message = {
            "role": "assistant",
            "content": f"I'll help you examine the file. Let me read {file_path or 'the file'} for you.",
            "tool_calls": [{
                "id": f"call_{uuid.uuid4().hex[:24]}",
                "type": "function",
                "function": {
                    "name": "read_file",
                    "arguments": json.dumps({"path": file_path or "."})
                }
            }]
        }
        
        if req.stream:
            # æµå¼å“åº”
            async def _cline_stream():
                # å‘é€è§’è‰²
                yield f"data: {json.dumps({'id': completion_id, 'object': 'chat.completion.chunk', 'created': created_ts, 'model': model_id, 'choices': [{'index': 0, 'delta': {'role': 'assistant'}}]}, ensure_ascii=False)}\n\n"
                
                # å‘é€å†…å®¹
                if message['content']:
                    yield f"data: {json.dumps({'id': completion_id, 'object': 'chat.completion.chunk', 'created': created_ts, 'model': model_id, 'choices': [{'index': 0, 'delta': {'content': message['content']}}]}, ensure_ascii=False)}\n\n"
                
                # å‘é€å·¥å…·è°ƒç”¨
                for i, tool_call in enumerate(message['tool_calls']):
                    yield f"data: {json.dumps({'id': completion_id, 'object': 'chat.completion.chunk', 'created': created_ts, 'model': model_id, 'choices': [{'index': 0, 'delta': {'tool_calls': [{'index': i, 'id': tool_call['id'], 'type': tool_call['type'], 'function': tool_call['function']}]}}]}, ensure_ascii=False)}\n\n"
                
                # å®Œæˆ
                yield f"data: {json.dumps({'id': completion_id, 'object': 'chat.completion.chunk', 'created': created_ts, 'model': model_id, 'choices': [{'index': 0, 'delta': {}, 'finish_reason': 'tool_calls'}]}, ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(_cline_stream(), media_type="text/event-stream")
        else:
            # éæµå¼å“åº”
            return {
                "id": completion_id,
                "object": "chat.completion",
                "created": created_ts,
                "model": model_id,
                "choices": [{
                    "index": 0,
                    "message": message,
                    "finish_reason": "tool_calls"
                }]
            }
    
    with PerformanceTracker("chat_completions"):
        # è®¤è¯æ£€æŸ¥
        if request:
            await authenticate_request(request)

        # å¼‚æ­¥åˆå§‹åŒ–ï¼Œä¸é˜»å¡ä¸»æµç¨‹
        try:
            initialize_once()
        except Exception as e:
            logger.warning(f"[OpenAI Compat] initialize_once failed or skipped: {e}")

        if not req.messages:
            raise HTTPException(400, "messages ä¸èƒ½ä¸ºç©º")
        
        # ç”Ÿæˆè¯·æ±‚ç¼“å­˜é”®ï¼ˆä»…å¯¹éæµå¼è¯·æ±‚ç¼“å­˜ï¼‰
        cache_key = None
        if not req.stream:
            cache_key = {
                "messages": [m.dict() for m in req.messages],
                "model": req.model,
                "temperature": getattr(req, 'temperature', None),
                "max_tokens": getattr(req, 'max_tokens', None),
                "tools": [t.dict() for t in req.tools] if req.tools else None,
            }
            
            # æ£€æŸ¥ç¼“å­˜
            cached_response = await cache_get(cache_key, ttl=120.0)  # 2åˆ†é’Ÿç¼“å­˜
            if cached_response:
                logger.info("[OpenAI Compat] Using cached response")
                return cached_response

    # 1) æ‰“å°æ¥æ”¶åˆ°çš„ Chat Completions åŸå§‹è¯·æ±‚ä½“
    try:
        logger.info("[OpenAI Compat] æ¥æ”¶åˆ°çš„ Chat Completions è¯·æ±‚ä½“(åŸå§‹): %s", json.dumps(req.dict(), ensure_ascii=False))
    except Exception:
        logger.info("[OpenAI Compat] æ¥æ”¶åˆ°çš„ Chat Completions è¯·æ±‚ä½“(åŸå§‹) åºåˆ—åŒ–å¤±è´¥")

    # æ£€æµ‹æ˜¯å¦æ˜¯ IDE å·¥å…·è¯·æ±‚ï¼ˆCline, Roo Code, Kilo Code ç­‰ï¼‰
    is_ide_tool_request = False
    ide_tool_name = None
    requested_file_path = None
    requested_tool_name = None
    requested_tool_args = None
    
    # æ£€æŸ¥ User-Agent
    if request:
        user_agent = request.headers.get("user-agent", "").lower()
        for tool in ["cline", "roo", "kilo", "cursor", "copilot"]:
            if tool in user_agent:
                is_ide_tool_request = True
                ide_tool_name = tool
                logger.info(f"[OpenAI Compat] Detected {tool} in User-Agent: {user_agent}")
                break
    
    # å¦‚æœæ²¡æœ‰ User-Agentï¼Œä½†æœ‰ Cline ç‰¹å¾çš„æ¨¡å‹å
    if not is_ide_tool_request and req.model and "claude" in req.model.lower():
        # æ£€æŸ¥æ˜¯å¦æœ‰ Cline ç‰¹å¾çš„æ¶ˆæ¯æ¨¡å¼
        for msg in req.messages:
            if msg.role == "user" and isinstance(msg.content, str):
                if any(keyword in msg.content.lower() for keyword in [
                    "php", "python", "javascript", "typescript", "java", "code", 
                    "file", "function", "class", "method", "variable",
                    "ä»£ç ", "æ–‡ä»¶", "å‡½æ•°", "ç±»", "æ–¹æ³•", "å˜é‡"
                ]):
                    is_ide_tool_request = True
                    ide_tool_name = "unknown-ide"
                    logger.info(f"[OpenAI Compat] Detected code-related request, assuming IDE tool")
                    break
    
    # æ£€æŸ¥æ¶ˆæ¯å†…å®¹ä¸­çš„å·¥å…·è°ƒç”¨æ¨¡å¼
    for msg in req.messages:
        if msg.role == "user" and isinstance(msg.content, str):
            content = msg.content
            logger.info(f"[OpenAI Compat] Checking user message: {content[:200]}...")  # æ‰“å°å‰200ä¸ªå­—ç¬¦
            
            # æ£€æµ‹å„ç§å·¥å…·è°ƒç”¨æ¨¡å¼
            
            # æ–‡ä»¶è¯»å–æ¨¡å¼
            file_patterns = [
                # Cline çš„æ ‡å‡†æ ¼å¼ï¼Œæ”¯æŒå†’å·åæœ‰ç©ºæ ¼ã€æ¢è¡Œæˆ–ç›´æ¥è·Ÿæ–‡ä»¶è·¯å¾„
                (r'(Cline|Roo|Kilo|Cursor) wants to read this file:\s*([^\n]+)', 1, 2),
                # æ”¯æŒæ²¡æœ‰ç©ºæ ¼çš„æƒ…å†µ
                (r'(Cline|Roo|Kilo|Cursor) wants to read this file:([^\s]+)', 1, 2),
                # å‡½æ•°è°ƒç”¨æ ¼å¼
                (r'read_file\(["\'](.*?)["\']\)', None, 1),
                (r'Reading file:\s*(.+?)(?:\s|$)', None, 1),
                # ä¸­æ–‡æ¨¡å¼
                (r'è®©æˆ‘.*?æŸ¥çœ‹.*?(.+?\.\w+)', None, 1),
                (r'æ‚¨è¯´å¾—å¯¹.*?æŸ¥çœ‹.*?(.+?\.\w+)', None, 1),
                # æ›´å®½æ¾çš„åŒ¹é…ï¼Œä»…æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡ä»¶è·¯å¾„
                (r'(/[\w\./\-]+\.\w+)', None, 1),  # Unix è·¯å¾„
                (r'([a-zA-Z]:\\[\w\\/\-]+\.\w+)', None, 1),  # Windows è·¯å¾„
            ]
            
            # å…¶ä»–å·¥å…·æ¨¡å¼
            tool_patterns = [
                (r'(list_files|ls|dir)\s*\(([^)]*)\)', 1, 2),
                (r'(create_file|write_file)\s*\(["\'](.*?)["\']', 1, 2),
                (r'(run_command|execute|bash)\s*\(["\'](.*?)["\']', 1, 2),
                (r'(search|grep)\s*\(["\'](.*?)["\']', 1, 2),
            ]
            
            import re
            
            # ç‰¹æ®Šå¤„ç† Cline çš„æ ‡å‡†æ ¼å¼
            if "Cline wants to read this file:" in content:
                is_ide_tool_request = True
                ide_tool_name = "cline"
                # æå–æ–‡ä»¶è·¯å¾„ - åœ¨å†’å·åé¢çš„éƒ¨åˆ†
                file_part = content.split("Cline wants to read this file:")[1]
                # åˆ†è¡Œå¤„ç†ï¼Œå–ç¬¬ä¸€ä¸ªéç©ºè¡Œä½œä¸ºæ–‡ä»¶è·¯å¾„
                lines = file_part.strip().split('\n')
                requested_file_path = None
                for line in lines:
                    line = line.strip()
                    if line and not line.startswith('#') and not line.startswith('//'):
                        # å¤„ç†å¯èƒ½è¢«æˆªæ–­çš„è·¯å¾„ (e.g., /...trollers/)
                        if '...' in line:
                            # å°è¯•è¡¥å…¨è·¯å¾„
                            if line.endswith('.php'):
                                requested_file_path = line.strip('"\' ')
                            else:
                                requested_file_path = line.strip('"\' ')
                        else:
                            requested_file_path = line.strip('"\' ')
                        break
                
                if requested_file_path:
                    requested_tool_name = "read_file"
                    requested_tool_args = {"path": requested_file_path}
                    logger.info(f"[OpenAI Compat] Cline file request detected: '{requested_file_path}'")
                else:
                    logger.warning(f"[OpenAI Compat] Cline request detected but no file path found in: {file_part[:100]}")
                    is_ide_tool_request = False
            else:
                # å…¶ä»–æ¨¡å¼åŒ¹é…
                for pattern, tool_group, path_group in file_patterns:
                    match = re.search(pattern, content, re.IGNORECASE)
                    if match:
                        is_ide_tool_request = True
                        if tool_group and not ide_tool_name:
                            ide_tool_name = match.group(tool_group).lower()
                        requested_file_path = match.group(path_group).strip()
                        requested_tool_name = "read_file"
                        requested_tool_args = {"path": requested_file_path}
                        logger.info(f"[OpenAI Compat] IDE tool detected: {ide_tool_name or 'unknown'}, file: {requested_file_path}")
                        break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶è¯»å–ï¼Œæ£€æŸ¥å…¶ä»–å·¥å…·
            if not is_ide_tool_request:
                for pattern, tool_name_group, args_group in tool_patterns:
                    match = re.search(pattern, content, re.IGNORECASE)
                    if match:
                        is_ide_tool_request = True
                        requested_tool_name = match.group(tool_name_group)
                        args_str = match.group(args_group) if args_group <= match.lastindex else ""
                        # ç®€å•è§£æå‚æ•°
                        if requested_tool_name in ["list_files", "ls", "dir"]:
                            requested_tool_args = {"path": args_str or "."}
                        elif requested_tool_name in ["create_file", "write_file"]:
                            requested_tool_args = {"path": args_str, "content": ""}
                        elif requested_tool_name in ["run_command", "execute", "bash"]:
                            requested_tool_args = {"command": args_str}
                        elif requested_tool_name in ["search", "grep"]:
                            requested_tool_args = {"pattern": args_str}
                        logger.info(f"[OpenAI Compat] IDE tool detected: {requested_tool_name} with args: {requested_tool_args}")
                        break
            
            if is_ide_tool_request:
                break
            
            # å¦‚æœè¿˜æ²¡æœ‰æ£€æµ‹åˆ°ï¼Œå†æ£€æŸ¥ä¸€äº›æ›´å®½æ¾çš„æ¨¡å¼
            if not is_ide_tool_request:
                # æ£€æŸ¥æ˜¯å¦åŒ…å« PHP æ–‡ä»¶è·¯å¾„
                php_file_pattern = r'([/\\]?[\w\./\\\-]*Controller\.php)'
                match = re.search(php_file_pattern, content)
                if match:
                    is_ide_tool_request = True
                    ide_tool_name = "unknown-ide"
                    requested_file_path = match.group(1)
                    requested_tool_name = "read_file"
                    requested_tool_args = {"path": requested_file_path}
                    logger.info(f"[OpenAI Compat] Detected PHP file pattern: {requested_file_path}")
    
    if is_ide_tool_request:
        logger.info(f"[OpenAI Compat] Detected IDE tool request from: {ide_tool_name or 'unknown'}")
        logger.info(f"[OpenAI Compat] Tool details - name: {requested_tool_name}, args: {requested_tool_args}")
    else:
        logger.info(f"[OpenAI Compat] No IDE tool pattern detected, proceeding with normal processing")
        
        # æœ€ç»ˆåå¤‡ï¼šå¦‚æœæ¶ˆæ¯ä¸­åŒ…å«ä»»ä½•çœ‹èµ·æ¥åƒæ–‡ä»¶è·¯å¾„çš„å†…å®¹ï¼Œå‡è®¾æ˜¯ IDE è¯·æ±‚
        for msg in req.messages:
            if msg.role == "user" and isinstance(msg.content, str):
                content = msg.content.lower()
                if any(keyword in content for keyword in [
                    "php", "controller", "file", "read", "view", "code",
                    ".php", ".py", ".js", ".ts", ".java",
                    "release", "sheet"
                ]):
                    logger.info(f"[OpenAI Compat] Fallback: Detected code-related keywords, assuming IDE request")
                    is_ide_tool_request = True
                    ide_tool_name = "unknown-ide"
                    if not requested_tool_name:
                        # å°è¯•æå–ä»»ä½•çœ‹èµ·æ¥åƒæ–‡ä»¶è·¯å¾„çš„å†…å®¹
                        import re
                        file_path_pattern = r'([/\\][\w\./\\\-]+\.\w+)'
                        match = re.search(file_path_pattern, msg.content)
                        if match:
                            requested_file_path = match.group(1)
                            requested_tool_name = "read_file"
                            requested_tool_args = {"path": requested_file_path}
                            logger.info(f"[OpenAI Compat] Fallback: Found file path: {requested_file_path}")
                    break
    
    # æ•´ç†æ¶ˆæ¯
    history: List[ChatMessage] = reorder_messages_for_anthropic(list(req.messages))
    
    # æ¶ˆæ¯å»é‡ - ç§»é™¤é‡å¤çš„ç³»ç»Ÿæç¤ºå’Œç”¨æˆ·æ¶ˆæ¯
    history = _deduplicate_messages(history)
    
    # è®°å½•æ¶ˆæ¯æ•°é‡ä½†ä¸æˆªæ–­
    logger.info(f"[OpenAI Compat] Processing {len(history)} messages (no truncation)")

    # 2) æ‰“å°æ•´ç†åçš„è¯·æ±‚ä½“ï¼ˆpost-reorderï¼‰
    try:
        logger.info("[OpenAI Compat] æ•´ç†åçš„è¯·æ±‚ä½“(post-reorder): %s", json.dumps({
            **req.dict(),
            "messages": [m.dict() for m in history]
        }, ensure_ascii=False))
    except Exception:
        logger.info("[OpenAI Compat] æ•´ç†åçš„è¯·æ±‚ä½“(post-reorder) åºåˆ—åŒ–å¤±è´¥")

    system_prompt_text: Optional[str] = None
    try:
        chunks: List[str] = []
        for _m in history:
            if _m.role == "system":
                _txt = segments_to_text(normalize_content_to_list(_m.content))
                if _txt.strip():
                    chunks.append(_txt)
        if chunks:
            system_prompt_text = "\n\n".join(chunks)
    except Exception:
        system_prompt_text = None

    task_id = STATE.baseline_task_id or str(uuid.uuid4())
    packet = packet_template()
    packet["task_context"] = {
        "tasks": [{
            "id": task_id,
            "description": "",
            "status": {"in_progress": {}},
            "messages": map_history_to_warp_messages(history, task_id, None, False),
        }],
        "active_task_id": task_id,
    }

    # è®¾ç½®æ¨¡å‹é…ç½® - æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡ï¼Œä½¿ç”¨æ”¯æŒè§†è§‰çš„æ¨¡å‹
    packet.setdefault("settings", {}).setdefault("model_config", {})

    # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡å†…å®¹
    has_images = False
    for msg in history:
        if hasattr(msg, 'content') and isinstance(msg.content, list):
            for content_item in msg.content:
                if isinstance(content_item, dict) and content_item.get("type") in ["image", "image_url"]:
                    has_images = True
                    break

    # æ ¹æ®æ˜¯å¦åŒ…å«å›¾ç‰‡é€‰æ‹©åˆé€‚çš„æ¨¡å‹
    if has_images:
        print("[OpenAI Compat] Detected images, using vision-capable model")
        # å¯¹äºå›¾ç‰‡å¤„ç†ï¼Œä¼˜å…ˆä½¿ç”¨æ›´å¼ºçš„æ¨¡å‹
        if req.model in ["claude-4.1-opus", "claude-4-opus", "gpt-4o", "gpt-4.1"]:
            vision_model = req.model
        else:
            # é»˜è®¤ä½¿ç”¨claude-4.1-opuså¤„ç†å›¾ç‰‡ï¼ˆæœ€å¼ºçš„è§†è§‰æ¨¡å‹ï¼‰
            vision_model = "claude-4.1-opus"
        
        # ç¡®ä¿å›¾ç‰‡å¤„ç†çš„æ¨¡å‹é…ç½®å®Œæ•´
        packet["settings"]["model_config"] = {
            "base": vision_model,
            "planning": "gpt-5 (high reasoning)",
            "coding": "auto"  # ä¿®å¤ï¼šç¡®ä¿codingæ¨¡å‹ä¸ä¸ºç©º
        }
        
        logger.info(f"[OpenAI Compat] Vision model config: {packet['settings']['model_config']}")
    else:
        # æ–‡æœ¬å¤„ç†ä½¿ç”¨æŒ‡å®šæ¨¡å‹æˆ–é»˜è®¤æ¨¡å‹ï¼Œæ˜ å°„claude-3-sonnetåˆ°claude-4-sonnet
        model_mapping = {
            "claude-3-sonnet": "claude-4-sonnet",
            "claude-3-opus": "claude-4-opus", 
            "claude-3-haiku": "claude-4-sonnet",  # æ˜ å°„åˆ°å¯ç”¨æ¨¡å‹
            "gpt-4": "gpt-4o",
            "gpt-3.5-turbo": "claude-4-sonnet"
        }
        requested_model = req.model or "claude-4-sonnet"
        actual_model = model_mapping.get(requested_model, requested_model)
        
        # ç¡®ä¿æ¨¡å‹é…ç½®å®Œæ•´
        packet["settings"]["model_config"] = {
            "base": actual_model,
            "planning": "gpt-5 (high reasoning)",
            "coding": "auto"  # ä¿®å¤ï¼šç¡®ä¿codingæ¨¡å‹ä¸ä¸ºç©º
        }
        
        if requested_model != actual_model:
            logger.info(f"[OpenAI Compat] Mapped model {requested_model} -> {actual_model}")
        
        logger.info(f"[OpenAI Compat] Final model config: {packet['settings']['model_config']}")

    if STATE.conversation_id:
        packet.setdefault("metadata", {})["conversation_id"] = STATE.conversation_id

    attach_user_and_tools_to_inputs(packet, history, system_prompt_text)

    if req.tools:
        mcp_tools: List[Dict[str, Any]] = []
        for t in req.tools:
            if t.type != "function" or not t.function:
                continue
            mcp_tools.append({
                "name": t.function.name,
                "description": t.function.description or "",
                "input_schema": t.function.parameters or {},
            })
        if mcp_tools:
            packet.setdefault("mcp_context", {}).setdefault("tools", []).extend(mcp_tools)

    # 3) æ‰“å°è½¬æ¢æˆ protobuf JSON çš„è¯·æ±‚ä½“ï¼ˆå‘é€åˆ° bridge çš„æ•°æ®åŒ…ï¼‰
    try:
        logger.info("[OpenAI Compat] è½¬æ¢æˆ Protobuf JSON çš„è¯·æ±‚ä½“: %s", json.dumps(packet, ensure_ascii=False))
    except Exception:
        logger.info("[OpenAI Compat] è½¬æ¢æˆ Protobuf JSON çš„è¯·æ±‚ä½“ åºåˆ—åŒ–å¤±è´¥")

    created_ts = int(time.time())
    completion_id = str(uuid.uuid4())
    model_id = req.model or "warp-default"

    if req.stream:
        logger.info("[OpenAI Compat] Processing streaming request - simplified logic...")
        
        # å¦‚æœæ˜¯ IDE å·¥å…·è¯·æ±‚å¹¶ä¸”æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›å·¥å…·è°ƒç”¨
        if is_ide_tool_request and requested_tool_name and requested_tool_args:
            logger.info(f"[OpenAI Compat] IDE tool request detected from {ide_tool_name or 'unknown'}, tool: {requested_tool_name}, args: {requested_tool_args}")
            
            async def _ide_tool_stream():
                # å‘é€è§’è‰²ä¿¡æ¯
                first_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created_ts,
                    "model": model_id or "claude-4-sonnet",
                    "choices": [{"index": 0, "delta": {"role": "assistant"}}],
                }
                yield f"data: {json.dumps(first_chunk, ensure_ascii=False)}\n\n"
                
                # å‘é€æ–‡æœ¬å†…å®¹
                response_text = _get_tool_response_text(requested_tool_name, requested_tool_args)
                content_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created_ts,
                    "model": model_id or "claude-4-sonnet",
                    "choices": [{"index": 0, "delta": {"content": response_text}}],
                }
                yield f"data: {json.dumps(content_chunk, ensure_ascii=False)}\n\n"
                
                # å‘é€å·¥å…·è°ƒç”¨
                tool_call_id = f"call_{uuid.uuid4().hex[:24]}"
                tool_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created_ts,
                    "model": model_id or "claude-4-sonnet",
                    "choices": [{"index": 0, "delta": {
                        "tool_calls": [{
                            "index": 0,
                            "id": tool_call_id,
                            "type": "function",
                            "function": {
                                "name": requested_tool_name,
                                "arguments": json.dumps(requested_tool_args, ensure_ascii=False)
                            }
                        }]
                    }}],
                }
                yield f"data: {json.dumps(tool_chunk, ensure_ascii=False)}\n\n"
                
                # å‘é€å®Œæˆæ ‡è®°
                done_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created_ts,
                    "model": model_id or "claude-4-sonnet",
                    "choices": [{"index": 0, "delta": {}, "finish_reason": "tool_calls"}],
                }
                yield f"data: {json.dumps(done_chunk, ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(_ide_tool_stream(), media_type="text/event-stream", headers={"Cache-Control": "no-cache", "Connection": "keep-alive"})
        
        # ç®€åŒ–ï¼šç›´æ¥è¿›è¡Œæµå¼å¤„ç†ï¼Œä¸åšå¤æ‚çš„é¢„æ£€æŸ¥
        async def _agen():
            try:
                async for chunk in stream_openai_sse(packet, completion_id, created_ts, model_id):
                    yield chunk
            except Exception as e:
                logger.error(f"[OpenAI Compat] Stream generation failed: {e}")
                # å‘é€ç®€å•çš„é”™è¯¯å“åº”
                error_message = "I'm currently experiencing high demand. Please try again in a moment"
                
                # å‘é€è§’è‰²ä¿¡æ¯
                first_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created_ts,
                    "model": model_id or "claude-4-sonnet",
                    "choices": [{"index": 0, "delta": {"role": "assistant"}}],
                }
                yield f"data: {json.dumps(first_chunk, ensure_ascii=False)}\n\n"
                
                # å‘é€é”™è¯¯å†…å®¹
                content_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created_ts,
                    "model": model_id or "claude-4-sonnet",
                    "choices": [{"index": 0, "delta": {"content": error_message}}],
                }
                yield f"data: {json.dumps(content_chunk, ensure_ascii=False)}\n\n"
                
                # å‘é€å®Œæˆæ ‡è®°
                done_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created_ts,
                    "model": model_id or "claude-4-sonnet",
                    "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
                }
                yield f"data: {json.dumps(done_chunk, ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"
        
        # æ·»åŠ fallbackæœºåˆ¶ï¼šå¦‚æœSSEå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–çš„å“åº”
        try:
            return StreamingResponse(_agen(), media_type="text/event-stream", headers={"Cache-Control": "no-cache", "Connection": "keep-alive"})
        except Exception as sse_error:
            logger.error(f"[OpenAI Compat] SSE processing failed: {sse_error}, using simple fallback")
            
            # ä½¿ç”¨ç®€åŒ–çš„æµå¼å“åº”ä½œä¸ºå¤‡é€‰
            async def _simple_fallback():
                try:
                    # å‘é€ä¸€ä¸ªç®€å•çš„è¯·æ±‚åˆ°bridge
                    response = await _post_once()
                    bridge_data = response.json()
                    
                    # ä½¿ç”¨ç®€åŒ–çš„å“åº”å¤„ç†
                    simple_stream = create_simple_chat_response(bridge_data, model_id, stream=True)
                    async for chunk in simple_stream:
                        yield chunk
                        
                except Exception as fallback_error:
                    logger.error(f"[OpenAI Compat] Simple fallback also failed: {fallback_error}")
                    # æœ€ç»ˆå¤‡é€‰æ–¹æ¡ˆ
                    yield f"data: {json.dumps({
                        'id': completion_id,
                        'object': 'chat.completion.chunk',
                        'created': created_ts,
                        'model': model_id,
                        'choices': [{'index': 0, 'delta': {'content': 'I apologize for the technical issue. Please try your request again.'}}]
                    }, ensure_ascii=False)}\n\n"
                    yield f"data: {json.dumps({
                        'id': completion_id,
                        'object': 'chat.completion.chunk', 
                        'created': created_ts,
                        'model': model_id,
                        'choices': [{'index': 0, 'delta': {}, 'finish_reason': 'stop'}]
                    }, ensure_ascii=False)}\n\n"
                    yield "data: [DONE]\n\n"
            
            return StreamingResponse(_simple_fallback(), media_type="text/event-stream", headers={"Cache-Control": "no-cache", "Connection": "keep-alive"})

    # å¦‚æœæ˜¯ IDE å·¥å…·éæµå¼è¯·æ±‚å¹¶ä¸”æœ‰å·¥å…·è°ƒç”¨
    if is_ide_tool_request and requested_tool_name and requested_tool_args:
        logger.info(f"[OpenAI Compat] IDE tool non-stream request from {ide_tool_name or 'unknown'}, tool: {requested_tool_name}, args: {requested_tool_args}")
        
        tool_call_id = f"call_{uuid.uuid4().hex[:24]}"
        final = {
            "id": completion_id,
            "object": "chat.completion",
            "created": created_ts,
            "model": model_id,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": _get_tool_response_text(requested_tool_name, requested_tool_args),
                    "tool_calls": [{
                        "id": tool_call_id,
                        "type": "function",
                        "function": {
                            "name": requested_tool_name,
                            "arguments": json.dumps(requested_tool_args, ensure_ascii=False)
                        }
                    }]
                },
                "finish_reason": "tool_calls"
            }],
        }
        
        # è®°å½•è¯·æ±‚å¤„ç†æ—¶é—´
        processing_time = time.time() - request_start_time
        logger.info(f"[OpenAI Compat] IDE tool request processed in {processing_time:.3f}s")
        
        return final

    async def _post_once() -> httpx.Response:
        client = await get_shared_async_client()
        return await client.post(
            f"{BRIDGE_BASE_URL}/api/warp/send_stream",
            json={"json_data": packet, "message_type": "warp.multi_agent.v1.Request"},
            timeout=httpx.Timeout(180.0, connect=5.0),
        )

    try:
        resp = await _post_once()
        if resp.status_code == 429:
            try:
                # é¦–å…ˆå°è¯•æ ‡å‡†JWTåˆ·æ–°
                client = await get_shared_async_client()
                r = await client.post(f"{BRIDGE_BASE_URL}/api/auth/refresh", timeout=10.0)
                logger.warning("[OpenAI Compat] Bridge returned 429. Tried JWT refresh -> HTTP %s", getattr(r, 'status_code', 'N/A'))
                
                # å¦‚æœæ ‡å‡†åˆ·æ–°å¤±è´¥ï¼Œå°è¯•ç”³è¯·æ–°çš„åŒ¿åtoken
                if r.status_code != 200:
                    logger.info("[OpenAI Compat] Standard refresh failed, trying anonymous token...")
                    from warp2protobuf.core.auth import acquire_anonymous_access_token
                    new_token = await acquire_anonymous_access_token()
                    if new_token:
                        logger.info("âœ… Successfully acquired new anonymous token for non-stream request")
            except Exception as _e:
                logger.warning("[OpenAI Compat] JWT refresh attempt failed after 429: %s", _e)
            resp = await _post_once()
        if resp.status_code != 200:
            text = resp.text
            logger.error("[OpenAI Compat] Bridge error %s: %s", resp.status_code, text[:500])
            raise HTTPException(resp.status_code, f"bridge_error: {text}")
        
        try:
            bridge_resp = resp.json()
        except Exception as json_e:
            text = resp.text
            logger.error("[OpenAI Compat] Failed to parse bridge response as JSON: %s", json_e)
            logger.error("[OpenAI Compat] Raw response: %s", text[:1000])
            raise HTTPException(502, f"bridge_response_parse_error: {json_e}")
            
        # è®°å½•å“åº”ä¿¡æ¯ç”¨äºè°ƒè¯•
        logger.info("[OpenAI Compat] Bridge response received: %s", {
            "status": resp.status_code,
            "response_length": len(bridge_resp.get("response", "")),
            "events_count": len(bridge_resp.get("parsed_events", [])),
            "conversation_id": bridge_resp.get("conversation_id"),
            "task_id": bridge_resp.get("task_id")
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error("[OpenAI Compat] Bridge request failed: %s", e)
        raise HTTPException(502, f"bridge_unreachable: {e}")

    try:
        STATE.conversation_id = bridge_resp.get("conversation_id") or STATE.conversation_id
        ret_task_id = bridge_resp.get("task_id")
        if isinstance(ret_task_id, str) and ret_task_id:
            STATE.baseline_task_id = ret_task_id
    except Exception:
        pass

    tool_calls: List[Dict[str, Any]] = []
    try:
        parsed_events = bridge_resp.get("parsed_events", []) or []
        for ev in parsed_events:
            evd = ev.get("parsed_data") or ev.get("raw_data") or {}
            client_actions = evd.get("client_actions") or evd.get("clientActions") or {}
            actions = client_actions.get("actions") or client_actions.get("Actions") or []
            for action in actions:
                add_msgs = action.get("add_messages_to_task") or action.get("addMessagesToTask") or {}
                if not isinstance(add_msgs, dict):
                    continue
                for message in add_msgs.get("messages", []) or []:
                    tc = message.get("tool_call") or message.get("toolCall") or {}
                    call_mcp = tc.get("call_mcp_tool") or tc.get("callMcpTool") or {}
                    if isinstance(call_mcp, dict) and call_mcp.get("name"):
                        try:
                            args_obj = call_mcp.get("args", {}) or {}
                            args_str = json.dumps(args_obj, ensure_ascii=False)
                        except Exception:
                            args_str = "{}"
                        tool_calls.append({
                            "id": tc.get("tool_call_id") or str(uuid.uuid4()),
                            "type": "function",
                            "function": {"name": call_mcp.get("name"), "arguments": args_str},
                        })
    except Exception:
        pass

    if tool_calls:
        msg_payload = {"role": "assistant", "content": "", "tool_calls": tool_calls}
        finish_reason = "tool_calls"
    else:
        response_text = bridge_resp.get("response", "")
        
        # éªŒè¯å“åº”å†…å®¹ä¸ä¸ºç©º
        if not response_text or not response_text.strip():
            # å°è¯•ä»parsed_eventsä¸­æå–å“åº”æ–‡æœ¬
            try:
                parsed_events = bridge_resp.get("parsed_events", []) or []
                response_parts = []
                for ev in parsed_events:
                    evd = ev.get("parsed_data") or ev.get("raw_data") or {}
                    client_actions = evd.get("client_actions") or evd.get("clientActions") or {}
                    actions = client_actions.get("actions") or client_actions.get("Actions") or []
                    for action in actions:
                        # ä»add_messages_to_taskä¸­æå–agent_output
                        add_msgs = action.get("add_messages_to_task") or action.get("addMessagesToTask") or {}
                        if isinstance(add_msgs, dict):
                            for message in add_msgs.get("messages", []) or []:
                                agent_output = message.get("agent_output") or message.get("agentOutput") or {}
                                text_content = agent_output.get("text", "")
                                if text_content and text_content.strip():
                                    response_parts.append(text_content)
                
                response_text = "".join(response_parts).strip()
                
                if not response_text:
                    logger.warning("[OpenAI Compat] Empty response from bridge")
                    # å¦‚æœæ˜¯ IDE å·¥å…·è¯·æ±‚ï¼Œè¿”å›ä¸€ä¸ªåŸºæœ¬çš„å·¥å…·è°ƒç”¨å“åº”
                    if is_ide_tool_request:
                        logger.info("[OpenAI Compat] Returning default tool call for IDE request")
                        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°å…·ä½“å·¥å…·ï¼Œé»˜è®¤ä½¿ç”¨ list_files
                        if not requested_tool_name:
                            requested_tool_name = "list_files"
                            requested_tool_args = {"path": "."}
                        # è¿”å›å·¥å…·è°ƒç”¨ä½œä¸ºå“åº”
                        tool_calls.append({
                            "id": f"call_{uuid.uuid4().hex[:24]}",
                            "type": "function",
                            "function": {
                                "name": requested_tool_name,
                                "arguments": json.dumps(requested_tool_args, ensure_ascii=False)
                            }
                        })
                        response_text = _get_tool_response_text(requested_tool_name, requested_tool_args)
                    else:
                        response_text = "I apologize, but I encountered an issue generating a response. Please try again."
                    
            except Exception as e:
                logger.error(f"[OpenAI Compat] Failed to extract response from parsed_events: {e}")
                # å¦‚æœæ˜¯ IDE å·¥å…·è¯·æ±‚ï¼Œè¿”å›ä¸€ä¸ªåŸºæœ¬çš„å·¥å…·è°ƒç”¨å“åº”
                if is_ide_tool_request:
                    logger.info("[OpenAI Compat] Error occurred, returning default tool call for IDE request")
                    if not requested_tool_name:
                        requested_tool_name = "list_files"
                        requested_tool_args = {"path": "."}
                    tool_calls.append({
                        "id": f"call_{uuid.uuid4().hex[:24]}",
                        "type": "function",
                        "function": {
                            "name": requested_tool_name,
                            "arguments": json.dumps(requested_tool_args, ensure_ascii=False)
                        }
                    })
                    response_text = _get_tool_response_text(requested_tool_name, requested_tool_args)
                else:
                    response_text = "I apologize, but I encountered an issue generating a response. Please try again."
        
        # é¢å¤–çš„å†…å®¹éªŒè¯å’Œæ¸…ç†
        if response_text:
            # æ£€æŸ¥å¹¶è½¬æ¢å„ç§é”™è¯¯æ¶ˆæ¯ä¸ºæ ‡å‡†è‹±æ–‡å“åº”
            error_patterns = [
                ("é…é¢å·²ç”¨å°½", "I'm currently experiencing high demand. Please try again in a moment."),
                ("quota", "I'm currently experiencing high demand. Please try again in a moment."),
                ("æœåŠ¡æš‚æ—¶ä¸å¯ç”¨", "Service is temporarily unavailable. Please try again later."),
                ("æœåŠ¡æš‚ä¸å¯ç”¨", "Service is temporarily unavailable. Please try again later."),
                ("è¿æ¥è¶…æ—¶", "Request timed out. Please try again."),
                ("ç½‘ç»œé”™è¯¯", "Network error occurred. Please try again."),
                ("è®¤è¯å¤±è´¥", "Authentication failed. Please check your credentials."),
                ("æƒé™ä¸è¶³", "Insufficient permissions for this request."),
            ]
            
            for pattern, replacement in error_patterns:
                if pattern in response_text.lower():
                    response_text = replacement
                    break
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å½»åº•æ¸…ç†é”™è¯¯å‰ç¼€å’Œåç¼€
            import re
            
            # å®šä¹‰éœ€è¦æ¸…ç†çš„æ¨¡å¼ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼‰
            error_patterns_regex = [
                r'\n\nThis may indicate.*?$',  # åŒ¹é…ä»"\n\nThis may indicate"åˆ°ç»“å°¾çš„æ‰€æœ‰å†…å®¹
                r'This may indicate.*?guidance.*?\.?',  # åŒ¹é…æ•´ä¸ªé”™è¯¯è¯´æ˜
                r'This may indicate.*?steps.*?\.?',  # åŒ¹é…åŒ…å«stepsçš„é”™è¯¯è¯´æ˜
                r'\(e\.g\..*?\)\.?',  # åŒ¹é…æ‹¬å·ä¸­çš„ç¤ºä¾‹
                r'inability to use a tool properly.*?$',  # åŒ¹é…å·¥å…·ä½¿ç”¨é”™è¯¯è¯´æ˜
                r'which can be mitigated.*?$',  # åŒ¹é…ç¼“è§£å»ºè®®
                r'Try breaking down.*?steps.*?\.?',  # åŒ¹é…åˆ†è§£ä»»åŠ¡å»ºè®®
            ]
            
            # åº”ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¸…ç†
            for pattern in error_patterns_regex:
                response_text = re.sub(pattern, '', response_text, flags=re.DOTALL | re.IGNORECASE)
            
            # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦å’Œæ ‡ç‚¹
            response_text = re.sub(r'\s+', ' ', response_text)  # åˆå¹¶å¤šä¸ªç©ºæ ¼
            response_text = re.sub(r'\n\s*\n\s*\n', '\n\n', response_text)  # åˆå¹¶å¤šä¸ªæ¢è¡Œ
            response_text = response_text.strip(' .,ã€‚ï¼Œ\n')
            
            # æ¸…ç†æ®‹ç•™çš„è¿æ¥è¯å’Œæ ‡ç‚¹
            cleanup_patterns = [
                r'^\s*(or|and|which|that)\s+',  # å¼€å¤´çš„è¿æ¥è¯
                r'\s+(or|and|which|that)\s*$',  # ç»“å°¾çš„è¿æ¥è¯
                r'^\s*[,ï¼Œ.ã€‚]\s*',  # å¼€å¤´çš„æ ‡ç‚¹
                r'\s*[,ï¼Œ.ã€‚]\s*$',  # ç»“å°¾çš„æ ‡ç‚¹
            ]
            
            for pattern in cleanup_patterns:
                response_text = re.sub(pattern, '', response_text, flags=re.IGNORECASE)
            
            response_text = response_text.strip()
            
            # ç¡®ä¿å“åº”ä¸ä¸ºç©ºä¸”æœ‰æ„ä¹‰
            if not response_text.strip() or len(response_text.strip()) < 3:
                response_text = "I apologize, but I encountered an issue generating a response. Please try again."
        else:
            # å¦‚æœå®Œå…¨æ²¡æœ‰å“åº”æ–‡æœ¬
            response_text = "I apologize, but I encountered an issue generating a response. Please try again."
        
        msg_payload = {"role": "assistant", "content": response_text}
        finish_reason = "stop"

    final = {
        "id": completion_id,
        "object": "chat.completion",
        "created": created_ts,
        "model": model_id,
        "choices": [{"index": 0, "message": msg_payload, "finish_reason": finish_reason}],
    }
    
    # è®°å½•å“åº”ä»¥ä¾¿è°ƒè¯•
    logger.info(f"[OpenAI Compat] Sending response: role={msg_payload.get('role')}, content_length={len(msg_payload.get('content', ''))}, finish_reason={finish_reason}")
    
    # ç¼“å­˜éæµå¼å“åº”
    if cache_key and not req.stream:
        await cache_set(cache_key, final, ttl=120.0)  # ç¼“å­˜2åˆ†é’Ÿ
        logger.debug("[OpenAI Compat] Cached response for future requests")
    
    # è®°å½•è¯·æ±‚å¤„ç†æ—¶é—´
    processing_time = time.time() - request_start_time
    logger.info(f"[OpenAI Compat] Request processed in {processing_time:.3f}s")
    
    return final


@router.get("/v1/performance")
async def get_performance_metrics_endpoint():
    """è·å–å…¨é¢æ€§èƒ½æŒ‡æ ‡"""
    try:
        # æ”¶é›†æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡
        metrics = {
            "status": "ok",
            "timestamp": time.time(),
            "performance": await get_performance_summary(),
            "http_client": get_performance_metrics(),
            "cache": await cache_stats(),
            "memory": await get_memory_stats(),
            "batching": await get_batch_stats(),
            "logging": get_async_log_stats(),
            "rate_limiting": get_rate_limit_stats(),
            "circuit_breakers": get_all_circuit_breaker_stats(),
            "json_optimization": get_json_stats(),
            "compression": get_compression_stats(),
        }
        
        # æ·»åŠ tokenç®¡ç†ç»Ÿè®¡
        try:
            from warp2protobuf.core.token_cache import get_token_management_stats
            metrics["token_management"] = get_token_management_stats()
        except Exception as e:
            logger.warning(f"Failed to get token management stats: {e}")
            metrics["token_management"] = {"error": str(e)}
        
        # è®¡ç®—ç»¼åˆå¥åº·åˆ†æ•°
        health_score = calculate_health_score(metrics)
        metrics["health_score"] = health_score
        
        return metrics
        
    except Exception as e:
        logger.error(f"[OpenAI Compat] Failed to get performance metrics: {e}")
        raise HTTPException(500, f"Failed to get performance metrics: {str(e)}")


def calculate_health_score(metrics: Dict[str, Any]) -> Dict[str, Any]:
    """è®¡ç®—ç³»ç»Ÿå¥åº·åˆ†æ•°"""
    scores = {}
    total_score = 0
    weight_sum = 0
    
    # HTTPå®¢æˆ·ç«¯å¥åº·åˆ†æ•°ï¼ˆæƒé‡ï¼š20%ï¼‰
    http_metrics = metrics.get("http_client", {})
    if http_metrics.get("total_requests", 0) > 0:
        success_rate = 1 - (http_metrics.get("failed_requests", 0) / http_metrics["total_requests"])
        scores["http_health"] = success_rate * 100
        total_score += scores["http_health"] * 0.2
        weight_sum += 0.2
    
    # ç¼“å­˜å¥åº·åˆ†æ•°ï¼ˆæƒé‡ï¼š15%ï¼‰
    cache_metrics = metrics.get("cache", {})
    if cache_metrics.get("total_requests", 0) > 0:
        cache_score = cache_metrics.get("hit_rate", 0) * 100
        scores["cache_health"] = cache_score
        total_score += cache_score * 0.15
        weight_sum += 0.15
    
    # å†…å­˜å¥åº·åˆ†æ•°ï¼ˆæƒé‡ï¼š25%ï¼‰
    memory_metrics = metrics.get("memory", {})
    memory_usage = memory_metrics.get("memory_usage", {})
    if memory_usage:
        memory_percent = memory_usage.get("percent", 0)
        # å†…å­˜ä½¿ç”¨ç‡è¶Šä½åˆ†æ•°è¶Šé«˜ï¼ˆ90%ä»¥ä¸Šå¼€å§‹æ‰£åˆ†ï¼‰
        memory_score = max(0, 100 - max(0, memory_percent - 70) * 2)
        scores["memory_health"] = memory_score
        total_score += memory_score * 0.25
        weight_sum += 0.25
    
    # é™æµå¥åº·åˆ†æ•°ï¼ˆæƒé‡ï¼š10%ï¼‰
    rate_limit_metrics = metrics.get("rate_limiting", {})
    if rate_limit_metrics and "global_block_rate" in rate_limit_metrics:
        block_rate = rate_limit_metrics["global_block_rate"]
        # é˜»å¡ç‡è¶Šä½åˆ†æ•°è¶Šé«˜
        rate_limit_score = max(0, 100 - block_rate * 500)  # 20%é˜»å¡ç‡ = 0åˆ†
        scores["rate_limit_health"] = rate_limit_score
        total_score += rate_limit_score * 0.1
        weight_sum += 0.1
    
    # ç†”æ–­å™¨å¥åº·åˆ†æ•°ï¼ˆæƒé‡ï¼š15%ï¼‰
    circuit_metrics = metrics.get("circuit_breakers", {})
    if circuit_metrics:
        open_circuits = sum(1 for cb in circuit_metrics.values() if cb.get("state") == "open")
        total_circuits = len(circuit_metrics)
        if total_circuits > 0:
            circuit_score = max(0, 100 - (open_circuits / total_circuits) * 100)
            scores["circuit_breaker_health"] = circuit_score
            total_score += circuit_score * 0.15
            weight_sum += 0.15
    
    # JSONä¼˜åŒ–å¥åº·åˆ†æ•°ï¼ˆæƒé‡ï¼š10%ï¼‰
    json_metrics = metrics.get("json_optimization", {})
    if json_metrics.get("total_operations", 0) > 0:
        cache_hit_rate = json_metrics.get("cache_hit_rate", 0)
        avg_time = json_metrics.get("avg_serialization_time", 0)
        # ç¼“å­˜å‘½ä¸­ç‡é«˜ä¸”å¤„ç†æ—¶é—´çŸ­å¾—åˆ†é«˜
        json_score = (cache_hit_rate * 50) + max(0, 50 - avg_time * 10000)
        scores["json_health"] = min(100, json_score)
        total_score += scores["json_health"] * 0.1
        weight_sum += 0.1
    
    # å‹ç¼©å¥åº·åˆ†æ•°ï¼ˆæƒé‡ï¼š5%ï¼‰
    compression_metrics = metrics.get("compression", {})
    if compression_metrics.get("total_requests", 0) > 0:
        compression_rate = compression_metrics.get("compression_rate", 0)
        size_reduction = compression_metrics.get("size_reduction", 0)
        compression_score = (compression_rate * 50) + (size_reduction * 50)
        scores["compression_health"] = min(100, compression_score)
        total_score += scores["compression_health"] * 0.05
        weight_sum += 0.05
    
    # è®¡ç®—æ€»åˆ†
    overall_score = total_score / max(weight_sum, 1) if weight_sum > 0 else 0
    
    # å¥åº·ç­‰çº§
    if overall_score >= 90:
        health_level = "excellent"
    elif overall_score >= 80:
        health_level = "good"
    elif overall_score >= 70:
        health_level = "fair"
    elif overall_score >= 60:
        health_level = "poor"
    else:
        health_level = "critical"
    
    return {
        "overall_score": round(overall_score, 2),
        "health_level": health_level,
        "component_scores": scores,
        "recommendations": generate_health_recommendations(scores, metrics)
    }


def generate_health_recommendations(scores: Dict[str, float], metrics: Dict[str, Any]) -> List[str]:
    """ç”Ÿæˆå¥åº·å»ºè®®"""
    recommendations = []
    
    # HTTPå®¢æˆ·ç«¯å»ºè®®
    if scores.get("http_health", 100) < 80:
        recommendations.append("Consider increasing HTTP connection pool size or timeout settings")
    
    # ç¼“å­˜å»ºè®®
    if scores.get("cache_health", 100) < 60:
        recommendations.append("Cache hit rate is low - consider increasing cache TTL or size")
    
    # å†…å­˜å»ºè®®
    if scores.get("memory_health", 100) < 70:
        recommendations.append("High memory usage detected - consider memory optimization or scaling")
    
    # é™æµå»ºè®®
    if scores.get("rate_limit_health", 100) < 80:
        recommendations.append("High rate limiting activity - consider adjusting limits or scaling")
    
    # ç†”æ–­å™¨å»ºè®®
    if scores.get("circuit_breaker_health", 100) < 90:
        recommendations.append("Circuit breakers are open - check downstream service health")
    
    # JSONä¼˜åŒ–å»ºè®®
    if scores.get("json_health", 100) < 70:
        recommendations.append("JSON processing performance is suboptimal - consider using faster JSON library")
    
    # å‹ç¼©å»ºè®®
    if scores.get("compression_health", 100) < 60:
        recommendations.append("Compression efficiency is low - check compression settings")
    
    if not recommendations:
        recommendations.append("System is performing optimally")
    
    return recommendations


@router.get("/v1/health/detailed")
async def detailed_health_check():
    """è¯¦ç»†å¥åº·æ£€æŸ¥"""
    try:
        # åŸºæœ¬å¥åº·æ£€æŸ¥
        health_status = {
            "status": "ok",
            "timestamp": time.time(),
            "uptime": time.time() - time.time(),  # å°†åœ¨å®é™…ä½¿ç”¨ä¸­æ›¿æ¢ä¸ºå¯åŠ¨æ—¶é—´
        }
        
        # æ£€æŸ¥bridgeè¿æ¥
        try:
            client = await get_shared_async_client()
            resp = await client.get(f"{BRIDGE_BASE_URL}/healthz", timeout=5.0)
            health_status["bridge"] = {
                "status": "ok" if resp.status_code == 200 else "error",
                "response_time": resp.elapsed.total_seconds() if hasattr(resp, 'elapsed') else None,
                "status_code": resp.status_code
            }
        except Exception as e:
            health_status["bridge"] = {
                "status": "error",
                "error": str(e)
            }
        
        # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
        try:
            cache_metrics = await cache_stats()
            health_status["cache"] = {
                "status": "ok",
                "hit_rate": cache_metrics.get("hit_rate", 0),
                "size": cache_metrics.get("size", 0)
            }
        except Exception as e:
            health_status["cache"] = {
                "status": "error",
                "error": str(e)
            }
        
        # æ£€æŸ¥å†…å­˜çŠ¶æ€
        try:
            memory_metrics = await get_memory_stats()
            memory_usage = memory_metrics.get("memory_usage", {})
            health_status["memory"] = {
                "status": "ok" if memory_usage.get("percent", 0) < 90 else "warning",
                "usage_percent": memory_usage.get("percent", 0),
                "rss_mb": memory_usage.get("rss_mb", 0)
            }
        except Exception as e:
            health_status["memory"] = {
                "status": "error",
                "error": str(e)
            }
        
        return health_status
        
    except Exception as e:
        logger.error(f"[OpenAI Compat] Detailed health check failed: {e}")
        raise HTTPException(500, f"Health check failed: {str(e)}") 